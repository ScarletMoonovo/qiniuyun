# 后端聊天功能与 RAG 实现

该后端聊天功能实现了一个基于 WebSocket 的实时通信系统，并结合了 **检索增强生成（RAG）** 能力，通过向量化记忆检索来增强对话体验。

---

## 聊天系统架构

后端聊天系统基于 **WebSocket 架构**，用于处理用户与 AI 角色之间的实时双向通信。  
在 `chatLogic.go:66` 中的 **Chat 主函数** 管理整个会话生命周期，包括用户认证、消息处理和回复生成。

系统支持多种消息类型：

- **认证消息**：`auth`
- **文本消息**：`text`
- **语音消息**：`voice`

对应的响应类型包括：**流式增量（delta）消息**、**完整消息**、**结束信号（done）** 以及 **音频响应**。  
参考位置：`chatLogic.go:24-38`

---

## RAG 实现

RAG 的核心功能通过 **嵌入生成与向量检索系统** 来实现。当用户发送一条消息时，系统会依次执行以下步骤：

1. **生成向量嵌入**  
   将用户输入内容转换为向量表示，使用外部嵌入服务完成。  
   参考位置：`chatLogic.go:116`

2. **搜索角色记忆**  
   在角色的记忆集合中执行语义检索，使用生成的向量进行匹配。  
   参考位置：`chatLogic.go:119`

3. **增强上下文**  
   将检索到的记忆加入到对话上下文中，再传递给 LLM。  
   参考位置：`chatLogic.go:121`

---

## 向量嵌入系统

嵌入系统通过一个 **专用客户端** 与 Python 嵌入服务及 Qdrant 向量数据库进行通信。  
参考位置：`embedding.go:24-28`

### 嵌入生成

文本会通过 **POST 请求** 发送到 Python 嵌入服务，并返回 **384 维的向量**。  
参考位置：`embedding.go:38-62`

### 向量存储

- 每个角色对应一个独立的 **Qdrant Collection**，命名规则统一。  
  参考位置：`globalkey.go:13-14`
- 系统创建 collection 时，使用 **余弦相似度** 作为度量方式，并存储向量及其对应的文本。  
  参考位置：`embedding.go:64-89`

### 语义搜索

- 使用 **余弦相似度** 进行检索，阈值为 **0.65**。
- 最多返回 **5 条最相关的记忆**。
- 搜索结果包含相似度分数及原始文本内容。  
  参考位置：`embedding.go:19-22`, `embedding.go:91-110`

---

## LLM 上下文构建

`castHistory` 函数负责构建最终的提示词：

- 将角色的系统提示词与检索到的记忆、对话历史拼接在一起。
- 记忆部分使用明确分隔符格式化，并附加到系统提示词后。

参考位置：`chatLogic.go:196-221`, `chatLogic.go:200-208`

---

## 流式回复生成

- 系统使用 **OpenAI 流式 ChatCompletion API** 生成实时回复。  
  参考位置：`llm.go:173-185`
- 生成的回复块会立刻通过 **WebSocket** 以 delta 消息形式发送给客户端，实现即时显示。  
  参考位置：`chatLogic.go:126-145`

---

## 消息持久化

- 所有对话交互都会在 **事务中持久化存储到数据库**，确保用户消息和 AI 回复一致保存。  
  参考位置：`chatLogic.go:165-184`
- 系统最多保留最近 **10 条历史消息**，以平衡上下文完整性与性能。  
  参考位置：`chatLogic.go:38`

---

## 语音集成

- 对于语音交互，系统会调用 **TTS WebSocket 服务** 生成语音。  
  参考位置：`chatLogic.go:147-159`
- 语音生成过程会建立独立的 WebSocket 连接，将合成后的音频流回客户端。  
  参考位置：`chatLogic.go:223-269`

---

## 说明

- RAG 实现提供了 **语义记忆检索**，使 AI 角色能够保持一致的人设，并回忆相关的过往对话。
- 向量检索的 **阈值与返回数量** 可调，以平衡检索相关性与响应速度。
- 系统采用 **模块化设计**，将嵌入生成、向量存储与检索解耦，具备良好的扩展性与可维护性。
- 基于 **WebSocket 的低延迟架构**，保证了实时聊天体验。